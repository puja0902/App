{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puja0902/App/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUf_9G8c8RNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-files for keys(authors list), values(fos list) and ratings\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the paths for the output files\n",
        "output_keys_file = '/content/drive/MyDrive/keys.txt'\n",
        "output_values_file = '/content/drive/MyDrive/values.txt'\n",
        "output_ratings_file = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Write keys to the output file with a comma at the end of each line\n",
        "with open(output_keys_file, 'w') as keys_file:\n",
        "    keys_file.write(','.join(keys).replace(',', ',\\n'))\n",
        "\n",
        "# Iterate through values and ratings\n",
        "for i in range(max(len(v) for v in data.values())):\n",
        "    values = []\n",
        "    ratings = []\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if i < len(value):\n",
        "            values.append(list(value.keys())[i] + ',')\n",
        "            ratings.append(str(list(value.values())[i]) + ',')\n",
        "\n",
        "    # Write values to the output file\n",
        "    with open(output_values_file, 'a') as values_file:\n",
        "        values_file.write('\\n'.join(values) + '\\n')\n",
        "\n",
        "    # Write ratings to the output file\n",
        "    with open(output_ratings_file, 'a') as ratings_file:\n",
        "        ratings_file.write('\\n'.join(ratings) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. To make network of co-authors\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# 2. fos-to-authors and authors-to-fos code\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. authors their fos with weights\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store author FOS with aggregated weights\n",
        "author_fos_dict = {}\n",
        "\n",
        "# Specify the input file path\n",
        "input_file_path = '/content/drive/MyDrive/dblp_papers_v11.txt'  # Replace with your file path\n",
        "\n",
        "# Read and process each line in the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Load each line as a JSON object\n",
        "        paper = json.loads(line)\n",
        "\n",
        "        authors = paper.get('authors', [])\n",
        "        fos_list = paper.get('fos', [])\n",
        "\n",
        "        # Iterate through authors for each paper\n",
        "        for author in authors:\n",
        "            author_name = author['name']\n",
        "\n",
        "            if author_name not in author_fos_dict:\n",
        "                author_fos_dict[author_name] = {}\n",
        "\n",
        "            # Create a dictionary to store aggregated weights for each FOS for this author\n",
        "            aggregated_weights = {}\n",
        "\n",
        "            # Iterate through FOS for the paper and aggregate weights\n",
        "            for fos in fos_list:\n",
        "                fos_name = fos['name']\n",
        "                fos_weight = fos['w']\n",
        "\n",
        "                # Aggregate weights for the same FOS for this author\n",
        "                if fos_name in aggregated_weights:\n",
        "                    aggregated_weights[fos_name].append(fos_weight)\n",
        "                else:\n",
        "                    aggregated_weights[fos_name] = [fos_weight]\n",
        "\n",
        "            # Calculate the average weights for each FOS for this author\n",
        "            for fos, weights in aggregated_weights.items():\n",
        "                average_weight = sum(weights) / len(weights)\n",
        "                author_fos_dict[author_name][fos] = average_weight\n",
        "\n",
        "# Save the author FOS data with aggregated weights to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/weights.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_fos_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# 4. authors their fos with opinions\n",
        "\n",
        "import json\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_opinions = {}\n",
        "    for concept, weight in concepts.items():\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            author_opinions[concept] = opinion\n",
        "    output_data[author] = author_opinions\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/opinion.json'\n",
        "\n",
        "# Read input JSON from the file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Create a new dictionary for the desired output\n",
        "output_dict = {}\n",
        "\n",
        "# Iterate through the input data and populate the output dictionary\n",
        "for author, fos_data in data.items():\n",
        "    for fos, value in fos_data.items():\n",
        "        if fos not in output_dict:\n",
        "            output_dict[fos] = {}\n",
        "        output_dict[fos][author] = value\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinionItem.json'\n",
        "\n",
        "# Write the output JSON to a file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get Authors list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getAuthors.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "#get fos list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/fos_to_authors.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getFos.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the key (Author) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/keys.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Fos) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/values.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testFos.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Rating) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the file paths for the JSON and TXT files\n",
        "json_file_paths = ['/content/drive/MyDrive/smallauthors_to_fos.json', '/content/drive/MyDrive/smallopinion.json',\n",
        "                   '/content/drive/MyDrive/smallfos_to_authors.json', '/content/drive/MyDrive/smallopinionItem.json']\n",
        "# txt_file_paths = ['/content/drive/MyDrive/trainAuthor.txt', '/content/drive/MyDrive/trainFos.txt', '/content/drive/MyDrive/trainRating.txt',\n",
        "#                   '/content/drive/MyDrive/testAuthor.txt', '/content/drive/MyDrive/testFos.txt', '/content/drive/MyDrive/testRating.txt']\n",
        "\n",
        "\n",
        "# Specify the output file path for the combined data\n",
        "output_file_path = '/content/drive/MyDrive/smallcombinedFile.json'\n",
        "\n",
        "# Initialize an empty list to store individual file data\n",
        "all_data = []\n",
        "\n",
        "# Read data from JSON files\n",
        "for json_path in json_file_paths:\n",
        "    with open(json_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    all_data.append(data)\n",
        "\n",
        "# Read data from TXT files\n",
        "# for txt_path in txt_file_paths:\n",
        "#     with open(txt_path, 'r') as txt_file:\n",
        "#         data = {'text_data': txt_file.read()}\n",
        "#     all_data.append(data)\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined data to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(all_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the file paths for the text files\n",
        "file1_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "file2_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "file3_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "file4_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "file5_path = '/content/drive/MyDrive/testFos.txt'\n",
        "file6_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file1_path, 'r') as file1:\n",
        "    data1 = [word.strip() for word in file1.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file2_path, 'r') as file2:\n",
        "    data2 = [word.strip() for word in file2.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file3_path, 'r') as file3:\n",
        "    data3 = [word.strip() for word in file3.read().split(',')]\n",
        "\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file4_path, 'r') as file4:\n",
        "    data4 = [word.strip() for word in file4.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file5_path, 'r') as file5:\n",
        "    data5 = [word.strip() for word in file5.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file6_path, 'r') as file6:\n",
        "    data6 = [word.strip() for word in file6.read().split(',')]\n",
        "\n",
        "\n",
        "# Combine the three lists into a single list\n",
        "combined_data = [data1, data2, data3, data4, data5, data6]\n",
        "\n",
        "# Write the combined data to the output file\n",
        "output_file_path = '/content/drive/MyDrive/combinedFile1.txt'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for item in combined_data:\n",
        "        output_file.write('[' + ',\\n'.join(item) + '],\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Read the content from the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace [ with { and ] with }\n",
        "modified_content = content.replace('[', '\"{').replace(']', '}\"')\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_file_path = '/content/drive/MyDrive/author_network.json'\n",
        "\n",
        "# Write the modified content back to the output file\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(modified_content)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert json to pickle\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/smalltestAuthor.json'\n",
        "input_file_path1 = '/content/drive/MyDrive/smalltestFos.json'\n",
        "input_file_path2 = '/content/drive/MyDrive/smalltestRating.json'\n",
        "input_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.json'\n",
        "input_file_path4 = '/content/drive/MyDrive/smalltrainFos.json'\n",
        "input_file_path5 = '/content/drive/MyDrive/smalltrainRating.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "pickle_file_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "pickle_file_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "pickle_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "pickle_file_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "pickle_file_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(input_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path1, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path2, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path3, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path4, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path5, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path2, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path3, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path4, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path5, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path = '/content/drive/MyDrive/combinedFile-edited.txt'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/combinedFile-edited.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path1, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "py_file_location = \"/content/drive/MyDrive/GraphRec-WWW19\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "\n",
        "# covert txt to json\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_txt_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "input_txt_path1 = '/content/drive/MyDrive/trainFos.txt'\n",
        "input_txt_path2 = '/content/drive/MyDrive/trainRating.txt'\n",
        "input_txt_path3 = '/content/drive/MyDrive/testAuthor.txt'\n",
        "input_txt_path4 = '/content/drive/MyDrive/testFos.txt'\n",
        "input_txt_path5 = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_json_path = '/content/drive/MyDrive/trainAuthor.json'\n",
        "output_json_path1 = '/content/drive/MyDrive/trainFos.json'\n",
        "output_json_path2 = '/content/drive/MyDrive/trainRating.json'\n",
        "output_json_path3 = '/content/drive/MyDrive/testAuthor.json'\n",
        "output_json_path4 = '/content/drive/MyDrive/testFos.json'\n",
        "output_json_path5 = '/content/drive/MyDrive/testRating.json'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(input_txt_path, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path1, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path2, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path3, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path4, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path5, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "# Write the list to a JSON file\n",
        "with open(output_json_path, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path1, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path2, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path3, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path4, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path5, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# Specify the paths for the input text files\n",
        "text_file_paths = [\n",
        "    '/content/drive/MyDrive/testAuthor.txt',\n",
        "    '/content/drive/MyDrive/testFos.txt',\n",
        "    '/content/drive/MyDrive/testRating.txt'\n",
        "]\n",
        "\n",
        "# Specify the path for the output merged text file\n",
        "output_text_path = '/content/drive/MyDrive/testCombined.txt'\n",
        "\n",
        "# Initialize an empty list to store the merged content\n",
        "merged_content = []\n",
        "\n",
        "# Read data from each text file and merge it\n",
        "for text_path in text_file_paths:\n",
        "    with open(text_path, 'r') as text_file:\n",
        "        content = text_file.read()\n",
        "        merged_content.append(content)\n",
        "\n",
        "# Write the merged content to the output text file\n",
        "with open(output_text_path, 'w') as output_file:\n",
        "    for content in merged_content:\n",
        "        output_file.write(content)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "text_file_path = '/content/drive/MyDrive/testFos.txt'\n",
        "text_file_path1 = '/content/drive/MyDrive/testRating.txt'\n",
        "text_file_path2 = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "text_file_path3 = '/content/drive/MyDrive/trainFos.txt'\n",
        "text_file_path4 = '/content/drive/MyDrive/trainRating.txt'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/testFos.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/testRating.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/trainAuthor.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/trainFos.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/trainRating.pkl'\n",
        "\n",
        "\n",
        "# Read data from the text file\n",
        "with open(text_file_path, 'r') as text_file:\n",
        "    text_data = text_file.read()\n",
        "\n",
        "with open(text_file_path1, 'r') as text_file:\n",
        "    text_data1 = text_file.read()\n",
        "\n",
        "with open(text_file_path2, 'r') as text_file:\n",
        "    text_data2 = text_file.read()\n",
        "\n",
        "with open(text_file_path3, 'r') as text_file:\n",
        "    text_data3 = text_file.read()\n",
        "\n",
        "with open(text_file_path4, 'r') as text_file:\n",
        "    text_data4 = text_file.read()\n",
        "\n",
        "# Convert the text data to a pickle object and write to the output file\n",
        "with open(output_pickle_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(text_data, output_pickle_file)\n",
        "\n",
        "with open(output_pickle_path1, 'wb') as output_pickle_file1:\n",
        "    pickle.dump(text_data1, output_pickle_file1)\n",
        "\n",
        "with open(output_pickle_path2, 'wb') as output_pickle_file2:\n",
        "    pickle.dump(text_data2, output_pickle_file2)\n",
        "\n",
        "with open(output_pickle_path3, 'wb') as output_pickle_file3:\n",
        "    pickle.dump(text_data3, output_pickle_file3)\n",
        "\n",
        "with open(output_pickle_path4, 'wb') as output_pickle_file4:\n",
        "    pickle.dump(text_data4, output_pickle_file4)\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "json_file_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/smallauthor_to_coauthors.pkl'\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Write the data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "text_file_path = '/content/drive/MyDrive/opinions.txt'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/opinions.pkl'\n",
        "\n",
        "\n",
        "\n",
        "# Read data from the text file\n",
        "with open(text_file_path, 'r') as text_file:\n",
        "    text_data = text_file.read()\n",
        "\n",
        "\n",
        "# Convert the text data to a pickle object and write to the output file\n",
        "with open(output_pickle_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(text_data, output_pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate Smaller- dataset\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Load dataset from a pickle file\n",
        "input_pickle_file_path = '/content/drive/MyDrive/testAuthor.pkl'\n",
        "input_pickle_file_path1 = '/content/drive/MyDrive/testFos.pkl'\n",
        "input_pickle_file_path2 = '/content/drive/MyDrive/testRating.pkl'\n",
        "input_pickle_file_path3 = '/content/drive/MyDrive/trainAuthor.pkl'\n",
        "input_pickle_file_path4 = '/content/drive/MyDrive/trainFos.pkl'\n",
        "input_pickle_file_path5 = '/content/drive/MyDrive/trainRating.pkl'\n",
        "input_pickle_file_path6 = '/content/drive/MyDrive/opinions.pkl'\n",
        "input_pickle_file_path7 = '/content/drive/MyDrive/combinedFile.pkl'\n",
        "\n",
        "with open(input_pickle_file_path, 'rb') as pickle_file:\n",
        "    df = pickle.load(pickle_file)\n",
        "\n",
        "with open(input_pickle_file_path1, 'rb') as pickle_file1:\n",
        "    df = pickle.load(pickle_file1)\n",
        "\n",
        "with open(input_pickle_file_path2, 'rb') as pickle_file2:\n",
        "    df = pickle.load(pickle_file2)\n",
        "\n",
        "with open(input_pickle_file_path3, 'rb') as pickle_file3:\n",
        "    df = pickle.load(pickle_file3)\n",
        "\n",
        "with open(input_pickle_file_path4, 'rb') as pickle_file4:\n",
        "    df = pickle.load(pickle_file4)\n",
        "\n",
        "with open(input_pickle_file_path5, 'rb') as pickle_file5:\n",
        "    df = pickle.load(pickle_file5)\n",
        "\n",
        "with open(input_pickle_file_path6, 'rb') as pickle_file6:\n",
        "    df = pickle.load(pickle_file6)\n",
        "\n",
        "with open(input_pickle_file_path7, 'rb') as pickle_file7:\n",
        "    df = pickle.load(pickle_file7)\n",
        "\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "percentage_to_keep = 0.4  # Adjust as needed\n",
        "smaller_df = random.sample(df, int(len(df) * percentage_to_keep))\n",
        "\n",
        "\n",
        "# Save the smaller DataFrame to a new pickle file\n",
        "output_pickle_file_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "output_pickle_file_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "output_pickle_file_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "output_pickle_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "output_pickle_file_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "output_pickle_file_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "output_pickle_file_path6 = '/content/drive/MyDrive/smallopinions.pkl'\n",
        "output_pickle_file_path7 = '/content/drive/MyDrive/smallcombinedFile.pkl'\n",
        "\n",
        "with open(output_pickle_file_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(smaller_df, output_pickle_file)\n",
        "\n",
        "with open(output_pickle_file_path1, 'wb') as output_pickle_file1:\n",
        "    pickle.dump(smaller_df, output_pickle_file1)\n",
        "\n",
        "with open(output_pickle_file_path2, 'wb') as output_pickle_file2:\n",
        "    pickle.dump(smaller_df, output_pickle_file2)\n",
        "\n",
        "with open(output_pickle_file_path3, 'wb') as output_pickle_file3:\n",
        "    pickle.dump(smaller_df, output_pickle_file3)\n",
        "\n",
        "with open(output_pickle_file_path4, 'wb') as output_pickle_file4:\n",
        "    pickle.dump(smaller_df, output_pickle_file4)\n",
        "\n",
        "with open(output_pickle_file_path5, 'wb') as output_pickle_file5:\n",
        "    pickle.dump(smaller_df, output_pickle_file5)\n",
        "\n",
        "with open(output_pickle_file_path6, 'wb') as output_pickle_file6:\n",
        "    pickle.dump(smaller_df, output_pickle_file6)\n",
        "\n",
        "with open(output_pickle_file_path7, 'wb') as output_pickle_file7:\n",
        "    pickle.dump(smaller_df, output_pickle_file7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller fragments of authors-to-co_authors\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 0.1%\n",
        "\n",
        "# Create a new dictionary to store the sampled data\n",
        "sampled_data = {}\n",
        "\n",
        "# Randomly sample each group of names\n",
        "for key, names in json_data.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data[key] = random.sample(names, num_to_keep)\n",
        "\n",
        "# Save the sampled data to a new JSON file\n",
        "output_json_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(sampled_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller fragments of author-to-fos and fos-to-author file\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON files\n",
        "input_json_path = '/content/drive/MyDrive/fos_to_authors.json'\n",
        "input_json_path1 = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "\n",
        "# Read the JSON data from the first file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data_fos_to_authors = json.load(input_file)\n",
        "\n",
        "# Read the JSON data from the second file\n",
        "with open(input_json_path1, 'r') as input_file:\n",
        "    json_data_authors_to_fos = json.load(input_file)\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Create new dictionaries to store the sampled data\n",
        "sampled_data_fos_to_authors = {}\n",
        "sampled_data_authors_to_fos = {}\n",
        "\n",
        "# Keep the first 40% of each group of names for the first file\n",
        "for key, names in json_data_fos_to_authors.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data_fos_to_authors[key] = names[:num_to_keep]\n",
        "\n",
        "# Keep the first 40% of each group of names for the second file\n",
        "for key, names in json_data_authors_to_fos.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data_authors_to_fos[key] = names[:num_to_keep]\n",
        "\n",
        "# Save the sampled data to new JSON files\n",
        "output_json_path_fos_to_authors = '/content/drive/MyDrive/smallfos_to_authors.json'\n",
        "with open(output_json_path_fos_to_authors, 'w') as output_file:\n",
        "    json.dump(sampled_data_fos_to_authors, output_file, indent=2)\n",
        "\n",
        "output_json_path_authors_to_fos = '/content/drive/MyDrive/smallauthors_to_fos.json'\n",
        "with open(output_json_path_authors_to_fos, 'w') as output_file:\n",
        "    json.dump(sampled_data_authors_to_fos, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON files\n",
        "input_json_path = '/content/drive/MyDrive/opinion.json'\n",
        "input_json_path1 = '/content/drive/MyDrive/opinionItem.json'\n",
        "\n",
        "# Read the JSON data from the first file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data_opinion = json.load(input_file)\n",
        "\n",
        "# Read the JSON data from the second file\n",
        "with open(input_json_path1, 'r') as input_file:\n",
        "    json_data_opinion_item = json.load(input_file)\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Create new dictionaries to store the sampled data\n",
        "sampled_data_opinion = {}\n",
        "sampled_data_opinion_item = {}\n",
        "\n",
        "# Keep the first 40% of each person's entries for the first file\n",
        "for person, entries in json_data_opinion.items():\n",
        "    num_to_keep = int(len(entries) * percentage_to_keep)\n",
        "    sampled_data_opinion[person] = dict(list(entries.items())[:num_to_keep])\n",
        "\n",
        "# Keep the first 40% of each person's entries for the second file\n",
        "for person, entries in json_data_opinion_item.items():\n",
        "    num_to_keep = int(len(entries) * percentage_to_keep)\n",
        "    sampled_data_opinion_item[person] = dict(list(entries.items())[:num_to_keep])\n",
        "\n",
        "# Save the sampled data to new JSON files\n",
        "output_json_path_opinion = '/content/drive/MyDrive/smallopinion.json'\n",
        "with open(output_json_path_opinion, 'w') as output_file:\n",
        "    json.dump(sampled_data_opinion, output_file, indent=2)\n",
        "\n",
        "output_json_path_opinion_item = '/content/drive/MyDrive/smallopinionItem.json'\n",
        "with open(output_json_path_opinion_item, 'w') as output_file:\n",
        "    json.dump(sampled_data_opinion_item, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/testFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/testRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/trainFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/trainRating.txt'\n",
        "# Specify the path for the new sampled text file\n",
        "output_text_path = '/content/drive/MyDrive/smalltestAuthor.txt'\n",
        "output_text_path1 = '/content/drive/MyDrive/smalltestFos.txt'\n",
        "output_text_path2 = '/content/drive/MyDrive/smalltestRating.txt'\n",
        "output_text_path3 = '/content/drive/MyDrive/smalltrainAuthor.txt'\n",
        "output_text_path4 = '/content/drive/MyDrive/smalltrainFos.txt'\n",
        "output_text_path5 = '/content/drive/MyDrive/smalltrainRating.txt'\n",
        "\n",
        "# Define the percentage to keep (40%)\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "\n",
        "# Calculate the number of names to keep\n",
        "num_names_to_keep = int(len(names) * percentage_to_keep)\n",
        "\n",
        "# Take the first 40% of names\n",
        "sampled_names = names[:num_names_to_keep]\n",
        "\n",
        "# Write the sampled names to the new text file\n",
        "with open(output_text_path, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path1, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path2, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path3, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path4, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path5, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/smalltestAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/smalltestFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/smalltestRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/smalltrainAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/smalltrainFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/smalltrainRating.txt'\n",
        "\n",
        "# Specify the path for the pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "output_pickle_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "# Create a dictionary with keys as indices and values as names\n",
        "indexed_names = {i: name for i, name in enumerate(names)}\n",
        "\n",
        "# Save the dictionary to a pickle file\n",
        "with open(output_pickle_path, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path1, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path2, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path3, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path4, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path5, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "\n",
        "\n",
        "# convert small training and testing txt files to json format.\n",
        "\n",
        "import json\n",
        "\n",
        "def text_to_json(input_file, output_file):\n",
        "    # Open the text file for reading\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Convert each line to a floating-point number\n",
        "    numbers = [float(line.strip().rstrip(',')) for line in lines]\n",
        "\n",
        "    # Write the numbers to a JSON file\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(numbers, json_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_files = ['/content/drive/MyDrive/smalltrainAuthor.txt', '/content/drive/MyDrive/smalltrainFos.txt', '/content/drive/MyDrive/smalltrainRating.txt',\n",
        "               '/content/drive/MyDrive/smalltestAuthor.txt', '/content/drive/MyDrive/smalltestFos.txt', '/content/drive/MyDrive/smalltestRating.txt']\n",
        "\n",
        "output_files = ['/content/drive/MyDrive/smalltrainAuthor.json', '/content/drive/MyDrive/smalltrainFos.json', '/content/drive/MyDrive/smalltrainRating.json',\n",
        "               '/content/drive/MyDrive/smalltestAuthor.json', '/content/drive/MyDrive/smalltestFos.json', '/content/drive/MyDrive/smalltestRating.json']\n",
        "# Convert each text file to JSON\n",
        "for input_file, output_file in zip(input_files, output_files):\n",
        "    text_to_json(input_file, output_file)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0iLSSXz_8R9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_json_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "# Calculate the number of items to keep (40%)\n",
        "percentage_to_keep = 0.4\n",
        "total_items = sum(len(names) for names in json_data.values())\n",
        "num_to_keep = int(total_items * percentage_to_keep)\n",
        "\n",
        "# Create a new dictionary to store the sampled data\n",
        "sampled_data = {}\n",
        "\n",
        "# Iterate through the keys and keep the first 40% of each list\n",
        "current_count = 0\n",
        "for key, names in json_data.items():\n",
        "    sampled_data[key] = names[:num_to_keep - current_count]\n",
        "    current_count += len(sampled_data[key])\n",
        "    if current_count >= num_to_keep:\n",
        "        break\n",
        "\n",
        "# Save the sampled data to a new JSON file\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(sampled_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "IAlho3ug8s01"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert authrs to co-authors json to pickle\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/smallauthor_to_coauthors.pkl'\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "# Save the data to a pickle file\n",
        "with open(output_pickle_path, 'wb') as output_file:\n",
        "    pickle.dump(json_data, output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "FpmjWltV9pgH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Attention.py\n",
        "\n"
      ],
      "metadata": {
        "id": "gBRDWAEN-Yry"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Aggregators.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UoemRjMG-dIY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Encoders.py\n",
        "\n"
      ],
      "metadata": {
        "id": "an7nyEYz-jH8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Aggregators.py\n"
      ],
      "metadata": {
        "id": "PLaQuY92-llO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Encoders.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dahwSZK9-n3C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test"
      ],
      "metadata": {
        "id": "heMJl3Y6-qzW",
        "outputId": "a1343c3f-eea0-4d9b-fde2-f27ae49d5928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 311, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 293, in main\n",
            "    train(graphrec, device, train_loader, optimizer, epoch, best_rmse, best_mae)\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 95, in train\n",
            "    loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 85, in loss\n",
            "    scores = self.forward(nodes_u, nodes_v)\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 66, in forward\n",
            "    embeds_u = self.enc_u(nodes_u)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Social_Encoders.py\", line 36, in forward\n",
            "    neigh_feats = self.aggregator.forward(nodes, to_neighs)  # user-user network\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Social_Aggregators.py\", line 41, in forward\n",
            "    u_rep = self.u2e.weight[nodes[i]]\n",
            "IndexError: index 4250310 is out of bounds for dimension 0 with size 2300037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "gQdMzK1MERqu",
        "outputId": "af8fd010-e824-4ce1-9384-efeaa0296ec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}