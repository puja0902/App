{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puja0902/App/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-files for keys(authors list), values(fos list) and ratings\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the paths for the output files\n",
        "output_keys_file = '/content/drive/MyDrive/keys.txt'\n",
        "output_values_file = '/content/drive/MyDrive/values.txt'\n",
        "output_ratings_file = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Write keys to the output file with a comma at the end of each line\n",
        "with open(output_keys_file, 'w') as keys_file:\n",
        "    keys_file.write(','.join(keys).replace(',', ',\\n'))\n",
        "\n",
        "# Iterate through values and ratings\n",
        "for i in range(max(len(v) for v in data.values())):\n",
        "    values = []\n",
        "    ratings = []\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if i < len(value):\n",
        "            values.append(list(value.keys())[i] + ',')\n",
        "            ratings.append(str(list(value.values())[i]) + ',')\n",
        "\n",
        "    # Write values to the output file\n",
        "    with open(output_values_file, 'a') as values_file:\n",
        "        values_file.write('\\n'.join(values) + '\\n')\n",
        "\n",
        "    # Write ratings to the output file\n",
        "    with open(output_ratings_file, 'a') as ratings_file:\n",
        "        ratings_file.write('\\n'.join(ratings) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. To make network of co-authors\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# 2. fos-to-authors and authors-to-fos code\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. authors their fos with weights\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store author FOS with aggregated weights\n",
        "author_fos_dict = {}\n",
        "\n",
        "# Specify the input file path\n",
        "input_file_path = '/content/drive/MyDrive/dblp_papers_v11.txt'  # Replace with your file path\n",
        "\n",
        "# Read and process each line in the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Load each line as a JSON object\n",
        "        paper = json.loads(line)\n",
        "\n",
        "        authors = paper.get('authors', [])\n",
        "        fos_list = paper.get('fos', [])\n",
        "\n",
        "        # Iterate through authors for each paper\n",
        "        for author in authors:\n",
        "            author_name = author['name']\n",
        "\n",
        "            if author_name not in author_fos_dict:\n",
        "                author_fos_dict[author_name] = {}\n",
        "\n",
        "            # Create a dictionary to store aggregated weights for each FOS for this author\n",
        "            aggregated_weights = {}\n",
        "\n",
        "            # Iterate through FOS for the paper and aggregate weights\n",
        "            for fos in fos_list:\n",
        "                fos_name = fos['name']\n",
        "                fos_weight = fos['w']\n",
        "\n",
        "                # Aggregate weights for the same FOS for this author\n",
        "                if fos_name in aggregated_weights:\n",
        "                    aggregated_weights[fos_name].append(fos_weight)\n",
        "                else:\n",
        "                    aggregated_weights[fos_name] = [fos_weight]\n",
        "\n",
        "            # Calculate the average weights for each FOS for this author\n",
        "            for fos, weights in aggregated_weights.items():\n",
        "                average_weight = sum(weights) / len(weights)\n",
        "                author_fos_dict[author_name][fos] = average_weight\n",
        "\n",
        "# Save the author FOS data with aggregated weights to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/weights.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_fos_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# 4. authors their fos with opinions\n",
        "\n",
        "import json\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_opinions = {}\n",
        "    for concept, weight in concepts.items():\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            author_opinions[concept] = opinion\n",
        "    output_data[author] = author_opinions\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/opinion.json'\n",
        "\n",
        "# Read input JSON from the file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Create a new dictionary for the desired output\n",
        "output_dict = {}\n",
        "\n",
        "# Iterate through the input data and populate the output dictionary\n",
        "for author, fos_data in data.items():\n",
        "    for fos, value in fos_data.items():\n",
        "        if fos not in output_dict:\n",
        "            output_dict[fos] = {}\n",
        "        output_dict[fos][author] = value\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinionItem.json'\n",
        "\n",
        "# Write the output JSON to a file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get Authors list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getAuthors.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "#get fos list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/fos_to_authors.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getFos.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the key (Author) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/keys.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Fos) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/values.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testFos.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Rating) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChfLWSXsU7nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the file paths for the JSON and TXT files\n",
        "json_file_paths = ['/content/drive/MyDrive/authors_to_fos.json', '/content/drive/MyDrive/opinion.json',\n",
        "                   '/content/drive/MyDrive/fos_to_authors.json', '/content/drive/MyDrive/opinionItem.json']\n",
        "# txt_file_paths = ['/content/drive/MyDrive/trainAuthor.txt', '/content/drive/MyDrive/trainFos.txt', '/content/drive/MyDrive/trainRating.txt',\n",
        "#                   '/content/drive/MyDrive/testAuthor.txt', '/content/drive/MyDrive/testFos.txt', '/content/drive/MyDrive/testRating.txt']\n",
        "\n",
        "\n",
        "# Specify the output file path for the combined data\n",
        "output_file_path = '/content/drive/MyDrive/combinedFile.json'\n",
        "\n",
        "# Initialize an empty list to store individual file data\n",
        "all_data = []\n",
        "\n",
        "# Read data from JSON files\n",
        "for json_path in json_file_paths:\n",
        "    with open(json_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    all_data.append(data)\n",
        "\n",
        "# Read data from TXT files\n",
        "# for txt_path in txt_file_paths:\n",
        "#     with open(txt_path, 'r') as txt_file:\n",
        "#         data = {'text_data': txt_file.read()}\n",
        "#     all_data.append(data)\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined data to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(all_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "--q5IDDoU_mV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file paths for the text files\n",
        "file1_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "file2_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "file3_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "file4_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "file5_path = '/content/drive/MyDrive/testFos.txt'\n",
        "file6_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file1_path, 'r') as file1:\n",
        "    data1 = [word.strip() for word in file1.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file2_path, 'r') as file2:\n",
        "    data2 = [word.strip() for word in file2.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file3_path, 'r') as file3:\n",
        "    data3 = [word.strip() for word in file3.read().split(',')]\n",
        "\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file4_path, 'r') as file4:\n",
        "    data4 = [word.strip() for word in file4.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file5_path, 'r') as file5:\n",
        "    data5 = [word.strip() for word in file5.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file6_path, 'r') as file6:\n",
        "    data6 = [word.strip() for word in file6.read().split(',')]\n",
        "\n",
        "\n",
        "# Combine the three lists into a single list\n",
        "combined_data = [data1, data2, data3, data4, data5, data6]\n",
        "\n",
        "# Write the combined data to the output file\n",
        "output_file_path = '/content/drive/MyDrive/combinedFile1.txt'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for item in combined_data:\n",
        "        output_file.write('[' + ',\\n'.join(item) + '],\\n')\n"
      ],
      "metadata": {
        "id": "89qfRIuFInfq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLTPD09dVGPT",
        "outputId": "6a0c36fb-2034-48c9-85cd-3d9cac717815"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Read the content from the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace [ with { and ] with }\n",
        "modified_content = content.replace('[', '\"{').replace(']', '}\"')\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_file_path = '/content/drive/MyDrive/author_network.json'\n",
        "\n",
        "# Write the modified content back to the output file\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(modified_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "_c-r3gQltkRy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert json to pickle\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "json_file_path = '/content/drive/MyDrive/combinedFile.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/combinedFile.pkl'\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "lFWhh4WQqAp_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path = '/content/drive/MyDrive/combinedFile-edited.txt'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/combinedFile-edited.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "2FnncWqYrkvU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path1, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "dGShWlB-3C0K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/Attention.py"
      ],
      "metadata": {
        "id": "UTvoLRmy7Z6J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Aggregators.py"
      ],
      "metadata": {
        "id": "EUZIP0Wq7CSc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Encoders.py"
      ],
      "metadata": {
        "id": "CwihWZLA7rVR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Aggregators.py"
      ],
      "metadata": {
        "id": "Vszbp9VG7wG6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Encoders.py"
      ],
      "metadata": {
        "id": "VlL8DzX470Aj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/run_GraphRec_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31vt30QO7_SL",
        "outputId": "a3510c7f-169f-4870-ec06-3a7059410507"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/run_GraphRec_example.py\", line 233, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/run_GraphRec_example.py\", line 156, in main\n",
            "    train_u, train_v, train_r, test_u, test_v, test_r = pickle.load(\n",
            "ValueError: too many values to unpack (expected 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "py_file_location = \"/content/drive/MyDrive/GraphRec-WWW19\"\n",
        "sys.path.append(os.path.abspath(py_file_location))"
      ],
      "metadata": {
        "id": "hlD6gVQp0JkI"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}